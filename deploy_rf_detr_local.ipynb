{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa402be7",
   "metadata": {},
   "source": [
    "# Deploying RF-DETR Model to SageMaker Using PyTorch\n",
    "\n",
    "This notebook demonstrates end-to-end deployment of the RF-DETR object detection model to Amazon SageMaker using PyTorch. The workflow includes using the pre-trained rf-detr-large checkpoint, creating custom inference code, packaging the model artifact, and deploying to a real-time SageMaker endpoint with GPU acceleration.\n",
    "\n",
    "This Notebook has been successfully tested on a Mac in VSCode and on Amazon SageMaker Studio.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2aea411",
   "metadata": {},
   "source": [
    "## Load Required Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45f0b95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "✔︎ JSON API cask.jws.json\n",
      "✔︎ JSON API formula.jws.json\n",
      "Warning: python@3.12 3.12.12 is already installed and up-to-date.\n",
      "To reinstall 3.12.12, run:\n",
      "  brew reinstall python@3.12\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "brew install python@3.12\n",
    "\n",
    "python3.12 -m pip install virtualenv --break-system-packages -Uq\n",
    "python3.12 -m venv .venv\n",
    "source .venv/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efd60605",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "# Use this version of pip to avoid compatibility issues\n",
    "python3.12 -m pip install pip==24.0 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323833bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker                          2.255.0\n",
      "sagemaker-core                     1.0.72\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "python3.12 -m pip uninstall sagemaker sagemaker-core sagemaker-mlflow sagemaker-mlops sagemaker-schema-inference-artifacts sagemaker-serve sagemaker-train -q -y\n",
    "python3.12 -m pip install -r requirements-deploy.txt -q\n",
    "python3.12 -m pip list | grep sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac0a638",
   "metadata": {},
   "source": [
    "## Initialize SageMaker Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9275394c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# CHANGE ME: Set to your existing Sagemaker Execution Role ARN\n",
    "os.environ[\"SAGEMAKER_ROLE_ARN\"] = (\n",
    "    \"arn:aws:iam::676164205626:role/service-role/AmazonSageMaker-ExecutionRole-20190927T120014\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e9b1af58-315f-4b73-b8b6-e4d5b185899b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Account ID: 676164205626\n",
      "RoleArn: arn:aws:iam::676164205626:role/service-role/AmazonSageMaker-ExecutionRole-20190927T120014\n",
      "Region: us-east-1\n",
      "Default S3 Bucket: sagemaker-us-east-1-676164205626\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import boto3\n",
    "\n",
    "# Create low-level clients\n",
    "session = boto3.Session()\n",
    "region = session.region_name or \"us-east-1\"\n",
    "sts = session.client(\"sts\", region_name=region)\n",
    "iam = session.client(\"iam\", region_name=region)\n",
    "sagemaker_client = session.client(\"sagemaker\", region_name=region)\n",
    "\n",
    "# Emulate SageMaker SDK's default bucket convention if you like\n",
    "role_arn = os.environ.get(\"SAGEMAKER_ROLE_ARN\")\n",
    "account_id = sts.get_caller_identity()[\"Account\"]\n",
    "default_bucket = f\"sagemaker-{region}-{account_id}\"\n",
    "\n",
    "print(f\"Account ID: {account_id}\")\n",
    "print(f\"RoleArn: {role_arn}\")\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"Default S3 Bucket: {default_bucket}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9975c33e",
   "metadata": {},
   "source": [
    "## Compress and Copy Model Model Artifacts to Amazon S3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82dced4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "VARIANTS = {\n",
    "    \"nano\": {\"file\": \"rf-detr-nano.pth\", \"type\": \"rfdetr-nano\", \"resolution\": \"384\"},\n",
    "    \"small\": {\"file\": \"rf-detr-small.pth\", \"type\": \"rfdetr-small\", \"resolution\": \"512\"},\n",
    "    \"medium\": {\n",
    "        \"file\": \"rf-detr-medium.pth\",\n",
    "        \"type\": \"rfdetr-medium\",\n",
    "        \"resolution\": \"576\",\n",
    "    },\n",
    "    \"base\": {\"file\": \"rf-detr-base.pth\", \"type\": \"rfdetr-base\", \"resolution\": \"560\"},\n",
    "    \"large\": {\"file\": \"rf-detr-large.pth\", \"type\": \"rfdetr-large\", \"resolution\": \"560\"},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86de04bf-7ec0-4b5d-9c1e-81b9b539e245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrain weights\n",
      "encoder='dinov2_windowed_base' out_feature_indexes=[2, 5, 8, 11] dec_layers=3 two_stage=True projector_scale=['P3', 'P5'] hidden_dim=384 patch_size=14 num_windows=4 sa_nheads=12 ca_nheads=24 dec_n_points=4 bbox_reparam=True lite_refpoint_refine=True layer_norm=True amp=True num_classes=90 pretrain_weights='rf-detr-large.pth' device='mps' resolution=560 group_detr=13 gradient_checkpointing=False positional_encoding_size=37 ia_bce_loss=True cls_loss_coef=1.0 segmentation_head=False mask_downsample_ratio=4 num_queries=300 num_select=300\n",
      "CPU times: user 2.85 s, sys: 1.15 s, total: 4 s\n",
      "Wall time: 3.21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from rfdetr import RFDETRLarge\n",
    "\n",
    "# Download the model 1x\n",
    "model = RFDETRLarge()\n",
    "print(model.model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f943831e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# various settings\n",
    "artifact_path = \"model.tar.gz\"\n",
    "key_prefix = \"pytorch-rf-detr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7fae9d58-e08a-4931-9228-50f72b7247bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 50.8 s, sys: 1.19 s, total: 52 s\n",
      "Wall time: 56.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Programmatically create a model artifact (tar.gz) containing the chosen weights and upload to S3\n",
    "import tarfile\n",
    "import os\n",
    "\n",
    "# Choose the weight file you want to package (one of the downloaded files)\n",
    "model_name = VARIANTS[\"large\"][\"file\"]\n",
    "weights_path = os.path.join(os.getcwd(), model_name)\n",
    "if not os.path.exists(weights_path):\n",
    "    raise FileNotFoundError(f\"Weights not found: {weights_path}\")\n",
    "\n",
    "with tarfile.open(artifact_path, \"w:gz\") as tar:\n",
    "    tar.add(weights_path, arcname=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd6fdba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded model artifact to: s3://sagemaker-us-east-1-676164205626/pytorch-rf-detr/model.tar.gz\n",
      "CPU times: user 3.67 s, sys: 2.56 s, total: 6.23 s\n",
      "Wall time: 40.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import os\n",
    "import boto3\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "file_name = os.path.basename(artifact_path)\n",
    "s3_key = f\"{key_prefix}/{file_name}\"\n",
    "\n",
    "# Takes 25-30 seconds on average\n",
    "s3.upload_file(artifact_path, default_bucket, s3_key)\n",
    "\n",
    "model_s3_path = f\"s3://{default_bucket}/{s3_key}\"\n",
    "\n",
    "print(f\"Uploaded model artifact to: {model_s3_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c47ab7",
   "metadata": {},
   "source": [
    "## Create the SageMaker Real-Time Endpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c38cdc-f22e-497d-8f12-c2929680de6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model_data: s3://sagemaker-us-east-1-676164205626/pytorch-rf-detr/model.tar.gz\n",
      "Deploying to endpoint: rfdetr-large-pytorch-2025-12-29-21-19-19-286016\n",
      "------------!CPU times: user 1min 5s, sys: 12.4 s, total: 1min 17s\n",
      "Wall time: 8min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from datetime import datetime, timezone\n",
    "import sagemaker\n",
    "\n",
    "\n",
    "# Reuse session from upload cell or create new one\n",
    "if \"session\" not in globals():\n",
    "    session = sagemaker.Session()\n",
    "\n",
    "print(f\"Using model_data: {model_data}\")\n",
    "\n",
    "image_uri = f\"763104351884.dkr.ecr.{region}.amazonaws.com/pytorch-inference:2.6.0-gpu-py312-cu124-ubuntu22.04-sagemaker-v1.56\"\n",
    "\n",
    "pytorch_model = PyTorchModel(\n",
    "    model_data=model_s3_path,\n",
    "    role=role_arn,\n",
    "    image_uri=image_uri,\n",
    "    entry_point=\"inference.py\",\n",
    "    source_dir=\"code\",\n",
    "    env={\n",
    "        \"RFDETR_VARIANT\": \"large\",\n",
    "        \"RFDETR_CONF\": \"0.25\",\n",
    "    },\n",
    ")\n",
    "\n",
    "instance_type = \"ml.g4dn.xlarge\"\n",
    "\n",
    "endpoint_name = (\n",
    "    VARIANTS[\"large\"][\"type\"]\n",
    "    + \"-pytorch-\"\n",
    "    + str(datetime.now(timezone.utc).strftime(\"%Y-%m-%d-%H-%M-%S-%f\"))\n",
    ")\n",
    "os.environ[\"SAGEMAKER_ENDPOINT_NAME\"] = endpoint_name\n",
    "print(f\"Deploying to endpoint: {endpoint_name}\")\n",
    "\n",
    "# Should take about 7-8 minutes to deploy\n",
    "predictor = pytorch_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    endpoint_name=endpoint_name,\n",
    "    deserializer=JSONDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223014f4-31ce-4006-8bc0-36b7fbe4e213",
   "metadata": {},
   "source": [
    "## Real-time Inference\n",
    "\n",
    "Perform real-time inference on the sample image directory and display the results with bounding box visualization. We are not passing any additional optional parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e3871bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = os.environ.get(\n",
    "    \"SAGEMAKER_ENDPOINT_NAME\", \"rfdetr-large-pytorch-2025-12-29-21-19-19-286016\"\n",
    ")\n",
    "rt = boto3.client(\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19da95f7-92ad-47ca-b9ba-f7a56880833f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18 images in sample_images\n",
      "Saved: sample_images_output/sample_images_01_detected.jpg\n",
      "Saved: sample_images_output/sample_images_02_detected.jpg\n",
      "Saved: sample_images_output/sample_images_03_detected.jpg\n",
      "Saved: sample_images_output/sample_images_04_detected.jpg\n",
      "Saved: sample_images_output/sample_images_05_detected.jpg\n",
      "Saved: sample_images_output/sample_images_06_detected.jpg\n",
      "Saved: sample_images_output/sample_images_07_detected.jpg\n",
      "Saved: sample_images_output/sample_images_08_detected.jpg\n",
      "Saved: sample_images_output/sample_images_09_detected.jpg\n",
      "Saved: sample_images_output/sample_images_10_detected.jpg\n",
      "Saved: sample_images_output/sample_images_11_detected.jpg\n",
      "Saved: sample_images_output/sample_images_12_detected.jpg\n",
      "Saved: sample_images_output/sample_images_13_detected.jpg\n",
      "Saved: sample_images_output/sample_images_14_detected.jpg\n",
      "Saved: sample_images_output/sample_images_15_detected.jpg\n",
      "Saved: sample_images_output/sample_images_16_detected.jpg\n",
      "Saved: sample_images_output/sample_images_17_detected.jpg\n",
      "Saved: sample_images_output/sample_images_18_detected.jpg\n",
      "Done. 18 images processed; results in: sample_images_output\n",
      "CPU times: user 917 ms, sys: 70.9 ms, total: 988 ms\n",
      "Wall time: 4.17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from io import BytesIO\n",
    "import json\n",
    "import random\n",
    "import glob\n",
    "import os\n",
    "\n",
    "\n",
    "base_dir = \"sample_images\"\n",
    "out_dir = \"sample_images_output\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "image_paths = sorted(\n",
    "    glob.glob(os.path.join(base_dir, \"*.jpg\"))\n",
    "    + glob.glob(os.path.join(base_dir, \"*.jpeg\"))\n",
    "    + glob.glob(os.path.join(base_dir, \"*.png\"))\n",
    ")\n",
    "\n",
    "if not image_paths:\n",
    "    raise FileNotFoundError(f\"No images found in {base_dir}\")\n",
    "\n",
    "print(f\"Found {len(image_paths)} images in {base_dir}\")\n",
    "\n",
    "font = ImageFont.load_default(size=24)\n",
    "IMG_SIZE = int(VARIANTS[\"large\"][\"resolution\"])\n",
    "\n",
    "\n",
    "def resize_long_side(image: Image.Image, max_size: int = 640) -> Image.Image:\n",
    "    w, h = image.size\n",
    "    long_side = max(w, h)\n",
    "    if long_side <= max_size:\n",
    "        return image  # no upscaling\n",
    "    scale = max_size / float(long_side)\n",
    "    new_w, new_h = int(w * scale), int(h * scale)\n",
    "    return image.resize((new_w, new_h), Image.Resampling.LANCZOS)\n",
    "\n",
    "\n",
    "for image_path in image_paths:\n",
    "    try:\n",
    "        orig_image = Image.open(image_path).convert(\"RGB\")\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping unreadable image: {image_path} - {e}\")\n",
    "        continue\n",
    "\n",
    "    # Downscale client-side: long side = 560, keep aspect ratio\n",
    "    send_image = resize_long_side(orig_image, IMG_SIZE)\n",
    "\n",
    "    buffer = BytesIO()\n",
    "    send_image.save(buffer, format=\"JPEG\", quality=90)\n",
    "    payload = buffer.getvalue()\n",
    "\n",
    "    # Send to endpoint\n",
    "    response = rt.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        ContentType=\"image/jpeg\",\n",
    "        Body=payload,\n",
    "    )\n",
    "    result = json.loads(response[\"Body\"].read().decode())\n",
    "    # print(json.dumps(result, indent=2))\n",
    "\n",
    "    # ASSUMPTION: boxes are in the coordinates of send_image\n",
    "    draw = ImageDraw.Draw(orig_image)\n",
    "\n",
    "    send_w, send_h = send_image.size\n",
    "    orig_w, orig_h = orig_image.size\n",
    "    x_ratio = orig_w / send_w\n",
    "    y_ratio = orig_h / send_h\n",
    "\n",
    "    for det in result.get(\"detections\", []):\n",
    "        x1, y1, x2, y2 = det[\"box\"]\n",
    "        conf = det[\"confidence\"]\n",
    "        label = det[\"label\"]\n",
    "\n",
    "        # Scale from send_image coords back to original\n",
    "        x1, x2 = int(x_ratio * x1), int(x_ratio * x2)\n",
    "        y1, y2 = int(y_ratio * y1), int(y_ratio * y2)\n",
    "\n",
    "        # color ranges: 10, 255 full, 10-100 dark, 150-225 light\n",
    "        color = (\n",
    "            random.randint(10, 225),\n",
    "            random.randint(10, 225),\n",
    "            random.randint(10, 225),\n",
    "        )\n",
    "\n",
    "        draw.rectangle([(x1, y1), (x2, y2)], outline=color, width=3)\n",
    "\n",
    "        text = f\"{label} ({int(conf * 100)}%)\"\n",
    "        draw.text((x1, max(0, y1 - 30)), text, fill=color, font=font)\n",
    "\n",
    "    base_name = os.path.basename(image_path)\n",
    "    name, ext = os.path.splitext(base_name)\n",
    "    out_path = os.path.join(out_dir, f\"{name}_detected{ext}\")\n",
    "    orig_image.save(out_path, quality=95)\n",
    "    print(f\"Saved: {out_path}\")\n",
    "\n",
    "print(f\"Done. {len(image_paths)} images processed; results in: {out_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38iauwr549",
   "metadata": {},
   "source": [
    "### Real-time Inference with JSON Format and Dynamic Confidence Threshold\n",
    "\n",
    "The inference endpoint supports a JSON request format that allows you to dynamically specify the confidence threshold per request. This is useful for:\n",
    "\n",
    "- Testing different confidence thresholds without redeploying\n",
    "- Adjusting sensitivity based on use case\n",
    "- A/B testing different threshold values\n",
    "\n",
    "**JSON Request Format:**\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"image\": \"base64_encoded_image_data\",\n",
    "  \"confidence\": 0.35 // Optional: defaults to 0.25 if not provided\n",
    "}\n",
    "```\n",
    "\n",
    "The example below demonstrates how to use the JSON format with different confidence thresholds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834ntzewrm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with image: sample_images/sample_images_03.jpg\n",
      "\n",
      "Testing with different confidence thresholds:\n",
      "\n",
      "Confidence: 0.25 | Detections: 22 | Inference time: 114.10063399989667 ms\n",
      "  → First detection: couch (0.946)\n",
      "Confidence: 0.50 | Detections:  8 | Inference time: 110.72942299961142 ms\n",
      "  → First detection: couch (0.946)\n",
      "Confidence: 0.90 | Detections:  2 | Inference time: 110.63572700004443 ms\n",
      "  → First detection: couch (0.946)\n",
      "Confidence: 0.94 | Detections:  1 | Inference time: 109.75208399986514 ms\n",
      "  → First detection: couch (0.946)\n",
      "\n",
      "Note: Higher confidence thresholds typically result in fewer but more confident detections.\n",
      "CPU times: user 42.5 ms, sys: 5.5 ms, total: 48 ms\n",
      "Wall time: 683 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import json\n",
    "import base64\n",
    "import glob\n",
    "import os\n",
    "\n",
    "\n",
    "# Load a sample image\n",
    "base_dir = \"sample_images\"\n",
    "image_paths = glob.glob(os.path.join(base_dir, \"*.jpg\"))[:1]  # Test with first image\n",
    "IMG_SIZE = int(VARIANTS[\"large\"][\"resolution\"])\n",
    "\n",
    "if not image_paths:\n",
    "    raise FileNotFoundError(f\"No images found in {base_dir}\")\n",
    "\n",
    "image_path = image_paths[0]\n",
    "print(f\"Testing with image: {image_path}\")\n",
    "\n",
    "# Load and prepare image\n",
    "orig_image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "\n",
    "# Downscale client-side: long side = 560, keep aspect ratio\n",
    "def resize_long_side(image: Image.Image, max_size: int = 560) -> Image.Image:\n",
    "    w, h = image.size\n",
    "    long_side = max(w, h)\n",
    "    if long_side <= max_size:\n",
    "        return image\n",
    "    scale = max_size / float(long_side)\n",
    "    new_w, new_h = int(w * scale), int(h * scale)\n",
    "    return image.resize((new_w, new_h), Image.Resampling.LANCZOS)\n",
    "\n",
    "\n",
    "send_image = resize_long_side(orig_image, IMG_SIZE)\n",
    "\n",
    "buffer = BytesIO()\n",
    "send_image.save(buffer, format=\"JPEG\", quality=90)\n",
    "image_bytes = buffer.getvalue()\n",
    "\n",
    "# Test with different confidence thresholds\n",
    "confidence_thresholds = [0.25, 0.5, 0.90, 0.94]\n",
    "\n",
    "print(f\"\\nTesting with different confidence thresholds:\\n\")\n",
    "\n",
    "for confidence in confidence_thresholds:\n",
    "    # Create JSON payload with base64-encoded image and confidence\n",
    "    payload = {\n",
    "        \"image\": base64.b64encode(image_bytes).decode(\"utf-8\"),\n",
    "        \"confidence\": confidence,\n",
    "    }\n",
    "\n",
    "    # Send to endpoint\n",
    "    response = rt.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        ContentType=\"application/json\",\n",
    "        Body=json.dumps(payload),\n",
    "    )\n",
    "    result = json.loads(response[\"Body\"].read().decode())\n",
    "\n",
    "    detection_count = result.get(\"metadata\", {}).get(\"count\", 0)\n",
    "    inference_time = result.get(\"metadata\", {}).get(\"inference_time_ms\", \"N/A\")\n",
    "\n",
    "    print(\n",
    "        f\"Confidence: {confidence:.2f} | Detections: {detection_count:2d} | Inference time: {inference_time} ms\"\n",
    "    )\n",
    "\n",
    "    # Print first detection for reference\n",
    "    if result.get(\"detections\"):\n",
    "        first_det = result[\"detections\"][0]\n",
    "        print(\n",
    "            f\"  → First detection: {first_det['label']} ({first_det['confidence']:.3f})\"\n",
    "        )\n",
    "\n",
    "print(\n",
    "    f\"\\nNote: Higher confidence thresholds typically result in fewer but more confident detections.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hlrkrm7b7u8",
   "metadata": {},
   "source": [
    "### Advanced Filtering: Classes, Max Detections, and Minimum Box Area\n",
    "\n",
    "In addition to dynamic confidence thresholds, the endpoint supports powerful post-processing filters:\n",
    "\n",
    "1. **`classes`**: Filter to specific object classes (e.g., only \"person\", \"car\"). Set to `null` to return all classes.\n",
    "2. **`max_detections`**: Limit the number of detections returned (returns top N by confidence)\n",
    "3. **`min_box_area`**: Filter out small detections below a minimum bounding box area (in pixels²)\n",
    "\n",
    "These filters are applied after inference, so they don't affect inference speed but can significantly reduce response payload size.\n",
    "\n",
    "**Example JSON Request:**\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"image\": \"base64_encoded_image\",\n",
    "  \"confidence\": 0.3,\n",
    "  \"classes\": [\"person\", \"car\"], // Only return people and cars (or null for all)\n",
    "  \"max_detections\": 10, // Return top 10 detections only\n",
    "  \"min_box_area\": 1000 // Filter out boxes smaller than 1000 px²\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vaho9chqgc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with image: sample_images/sample_images_03.jpg\n",
      "\n",
      "================================================================================\n",
      "Example 1: Return all classes (classes=null)\n",
      "================================================================================\n",
      "Detections: 22\n",
      "Classes found: ['book', 'bowl', 'potted plant', 'sink', 'chair', 'dining table', 'couch']\n",
      "\n",
      "================================================================================\n",
      "Example 2: Filter to specific classes (scissors, bottle, knife)\n",
      "================================================================================\n",
      "Detections: 0\n",
      "Classes found: ['book', 'bowl', 'potted plant', 'sink', 'chair', 'dining table', 'couch']\n",
      "Applied filters: {'confidence': 0.25, 'classes': ['scissors', 'bottle', 'knife']}\n",
      "\n",
      "================================================================================\n",
      "Example 3: Limit to top 5 detections by confidence\n",
      "================================================================================\n",
      "Detections returned: 5\n",
      "Applied filters: {'confidence': 0.2, 'max_detections': 5, 'total_before_limit': 26}\n",
      "Top detection: couch (0.946)\n",
      "\n",
      "================================================================================\n",
      "Example 4: Filter out small detections (min_box_area=8000 px²)\n",
      "================================================================================\n",
      "Detections (large objects only): 3\n",
      "  - couch: 42757.9 px² (conf: 0.946)\n",
      "  - chair: 16637.5 px² (conf: 0.918)\n",
      "  - couch: 16692.0 px² (conf: 0.308)\n",
      "Applied filters: {'confidence': 0.25, 'min_box_area': 8000.0}\n",
      "\n",
      "================================================================================\n",
      "Example 5: Combine all filters\n",
      "================================================================================\n",
      "Final detections: 0\n",
      "Applied filters: {'confidence': 0.2, 'classes': ['scissors', 'bottle', 'knife'], 'max_detections': 3, 'min_box_area': 1000.0}\n",
      "\n",
      "================================================================================\n",
      "Summary: Filters allow you to customize responses without redeploying!\n",
      "================================================================================\n",
      "CPU times: user 47.2 ms, sys: 6.14 ms, total: 53.3 ms\n",
      "Wall time: 839 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import json\n",
    "import base64\n",
    "import glob\n",
    "import os\n",
    "\n",
    "\n",
    "# Load a sample image\n",
    "base_dir = \"sample_images\"\n",
    "image_paths = glob.glob(os.path.join(base_dir, \"*.jpg\"))[:1]\n",
    "IMG_SIZE = int(VARIANTS[\"large\"][\"resolution\"])\n",
    "\n",
    "if not image_paths:\n",
    "    raise FileNotFoundError(f\"No images found in {base_dir}\")\n",
    "\n",
    "image_path = image_paths[0]\n",
    "print(f\"Testing with image: {image_path}\\n\")\n",
    "\n",
    "# Load and prepare image\n",
    "orig_image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "\n",
    "def resize_long_side(image: Image.Image, max_size: int = 560) -> Image.Image:\n",
    "    w, h = image.size\n",
    "    long_side = max(w, h)\n",
    "    if long_side <= max_size:\n",
    "        return image\n",
    "    scale = max_size / float(long_side)\n",
    "    new_w, new_h = int(w * scale), int(h * scale)\n",
    "    return image.resize((new_w, new_h), Image.Resampling.LANCZOS)\n",
    "\n",
    "\n",
    "send_image = resize_long_side(orig_image, IMG_SIZE)\n",
    "\n",
    "buffer = BytesIO()\n",
    "send_image.save(buffer, format=\"JPEG\", quality=90)\n",
    "image_bytes = buffer.getvalue()\n",
    "image_b64 = base64.b64encode(image_bytes).decode(\"utf-8\")\n",
    "\n",
    "# Example 1: Return all classes (no class filtering)\n",
    "print(\"=\" * 80)\n",
    "print(\"Example 1: Return all classes (classes=null)\")\n",
    "print(\"=\" * 80)\n",
    "payload = {\n",
    "    \"image\": image_b64,\n",
    "    \"confidence\": 0.25,\n",
    "    \"classes\": None,  # Explicitly request all classes\n",
    "}\n",
    "response = rt.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"application/json\",\n",
    "    Body=json.dumps(payload),\n",
    ")\n",
    "result1 = json.loads(response[\"Body\"].read().decode())\n",
    "print(f\"Detections: {result1['metadata']['count']}\")\n",
    "print(f\"Classes found: {list(set([d['label'] for d in result1['detections']]))}\")\n",
    "print()\n",
    "\n",
    "# Example 2: Filter to specific classes only\n",
    "print(\"=\" * 80)\n",
    "print(\"Example 2: Filter to specific classes (scissors, bottle, knife)\")\n",
    "print(\"=\" * 80)\n",
    "payload = {\n",
    "    \"image\": image_b64,\n",
    "    \"confidence\": 0.25,\n",
    "    \"classes\": [\"scissors\", \"bottle\", \"knife\"],\n",
    "}\n",
    "response = rt.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"application/json\",\n",
    "    Body=json.dumps(payload),\n",
    ")\n",
    "result2 = json.loads(response[\"Body\"].read().decode())\n",
    "print(f\"Detections: {result2['metadata']['count']}\")\n",
    "print(f\"Classes found: {list(set([d['label'] for d in result1['detections']]))}\")\n",
    "if result2[\"metadata\"].get(\"applied_filters\"):\n",
    "    print(f\"Applied filters: {result2['metadata']['applied_filters']}\")\n",
    "print()\n",
    "\n",
    "# Example 3: Limit to top 5 detections\n",
    "print(\"=\" * 80)\n",
    "print(\"Example 3: Limit to top 5 detections by confidence\")\n",
    "print(\"=\" * 80)\n",
    "payload = {\n",
    "    \"image\": image_b64,\n",
    "    \"confidence\": 0.2,  # Lower confidence to get more candidates\n",
    "    \"max_detections\": 5,\n",
    "}\n",
    "response = rt.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"application/json\",\n",
    "    Body=json.dumps(payload),\n",
    ")\n",
    "result3 = json.loads(response[\"Body\"].read().decode())\n",
    "print(f\"Detections returned: {result3['metadata']['count']}\")\n",
    "if result3[\"metadata\"].get(\"applied_filters\"):\n",
    "    print(f\"Applied filters: {result3['metadata']['applied_filters']}\")\n",
    "if result3[\"detections\"]:\n",
    "    print(\n",
    "        f\"Top detection: {result3['detections'][0]['label']} ({result3['detections'][0]['confidence']:.3f})\"\n",
    "    )\n",
    "print()\n",
    "\n",
    "# Example 4: Filter by minimum box area\n",
    "print(\"=\" * 80)\n",
    "print(\"Example 4: Filter out small detections (min_box_area=8000 px²)\")\n",
    "print(\"=\" * 80)\n",
    "payload = {\"image\": image_b64, \"confidence\": 0.25, \"min_box_area\": 8000}\n",
    "response = rt.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"application/json\",\n",
    "    Body=json.dumps(payload),\n",
    ")\n",
    "result4 = json.loads(response[\"Body\"].read().decode())\n",
    "print(f\"Detections (large objects only): {result4['metadata']['count']}\")\n",
    "if result4[\"detections\"]:\n",
    "    for det in result4[\"detections\"]:\n",
    "        print(\n",
    "            f\"  - {det['label']}: {det['area']:.1f} px² (conf: {det['confidence']:.3f})\"\n",
    "        )\n",
    "if result4[\"metadata\"].get(\"applied_filters\"):\n",
    "    print(f\"Applied filters: {result4['metadata']['applied_filters']}\")\n",
    "print()\n",
    "\n",
    "# Example 5: Combine all filters\n",
    "print(\"=\" * 80)\n",
    "print(\"Example 5: Combine all filters\")\n",
    "print(\"=\" * 80)\n",
    "payload = {\n",
    "    \"image\": image_b64,\n",
    "    \"confidence\": 0.2,\n",
    "    \"classes\": [\"scissors\", \"bottle\", \"knife\"],\n",
    "    \"max_detections\": 3,\n",
    "    \"min_box_area\": 1000,\n",
    "}\n",
    "response = rt.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"application/json\",\n",
    "    Body=json.dumps(payload),\n",
    ")\n",
    "result5 = json.loads(response[\"Body\"].read().decode())\n",
    "print(f\"Final detections: {result5['metadata']['count']}\")\n",
    "print(f\"Applied filters: {result5['metadata'].get('applied_filters', {})}\")\n",
    "for det in result5[\"detections\"]:\n",
    "    print(\n",
    "        f\"  - {det['label']}: area={det['area']:.1f}px², conf={det['confidence']:.3f}\"\n",
    "    )\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Summary: Filters allow you to customize responses without redeploying!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6921b8-ceda-461e-85c2-ea6eb01b7fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detections: 2\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import base64\n",
    "import supervision as sv\n",
    "import numpy as np\n",
    "\n",
    "# Load and prepare image\n",
    "orig_image = Image.open(\"sample_images_output/sample_images_01_detected.jpg\").convert(\n",
    "    \"RGB\"\n",
    ")\n",
    "\n",
    "IMG_SIZE = int(VARIANTS[\"large\"][\"resolution\"])\n",
    "\n",
    "\n",
    "def resize_long_side(image: Image.Image, max_size: int = 560) -> Image.Image:\n",
    "    w, h = image.size\n",
    "    long_side = max(w, h)\n",
    "    if long_side <= max_size:\n",
    "        return image\n",
    "    scale = max_size / float(long_side)\n",
    "    new_w, new_h = int(w * scale), int(h * scale)\n",
    "    return image.resize((new_w, new_h), Image.Resampling.LANCZOS)\n",
    "\n",
    "\n",
    "send_frame = resize_long_side(orig_image, IMG_SIZE)\n",
    "buffer = BytesIO()\n",
    "send_frame.save(buffer, format=\"JPEG\", quality=90)\n",
    "\n",
    "# Calculate scaling factors for box coordinates\n",
    "scale_x = orig_image.size[0] / send_frame.size[0]\n",
    "scale_y = orig_image.size[1] / send_frame.size[1]\n",
    "\n",
    "# Prepare and send request\n",
    "payload = {\n",
    "    \"image\": base64.b64encode(buffer.getvalue()).decode(\"utf-8\"),\n",
    "    \"confidence\": 0.5,\n",
    "}\n",
    "\n",
    "response = rt.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"application/json\",\n",
    "    Body=json.dumps(payload),\n",
    ")\n",
    "result = json.loads(response[\"Body\"].read().decode())\n",
    "\n",
    "print(f\"Detections: {result['metadata']['count']}\")\n",
    "\n",
    "# Rescale boxes to original image coordinates\n",
    "boxes_rescaled = [\n",
    "    [x1 * scale_x, y1 * scale_y, x2 * scale_x, y2 * scale_y]\n",
    "    for x1, y1, x2, y2 in [d[\"box\"] for d in result[\"detections\"]]\n",
    "]\n",
    "\n",
    "# Create detections and labels\n",
    "detections = sv.Detections(\n",
    "    xyxy=np.array(boxes_rescaled),\n",
    "    class_id=np.array([d[\"class_id\"] for d in result[\"detections\"]]),\n",
    "    confidence=np.array([d[\"confidence\"] for d in result[\"detections\"]]),\n",
    ")\n",
    "\n",
    "labels = [f\"{d['label']} {d['confidence'] * 100:.1f}%\" for d in result[\"detections\"]]\n",
    "\n",
    "# Annotate image\n",
    "annotated = sv.BoxAnnotator().annotate(\n",
    "    sv.LabelAnnotator(smart_position=True).annotate(\n",
    "        orig_image, detections=detections, labels=labels\n",
    "    ),\n",
    "    detections=detections,\n",
    ")\n",
    "\n",
    "# Save result\n",
    "annotated.save(\"sample_images_output/annotated_output.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7969ca",
   "metadata": {},
   "source": [
    "## Video Object Detection with Amazon SageMaker\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4821036",
   "metadata": {},
   "source": [
    "### Simple Video Object Detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb2c1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting video processing...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "add9aada81ae4eb197fd6afd8a43ac5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing video:   0%|          | 0/241 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[cb] frame 0, shape=(1080, 1920, 3)\n",
      "[cb] frame 50, shape=(1080, 1920, 3)\n",
      "[cb] frame 100, shape=(1080, 1920, 3)\n",
      "[cb] frame 150, shape=(1080, 1920, 3)\n",
      "[cb] frame 200, shape=(1080, 1920, 3)\n",
      "Processing complete.\n",
      "Final file size (bytes): 37610488\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import base64\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import supervision as sv\n",
    "\n",
    "SOURCE_VIDEO_PATH = \"sample_images/sample_video.mp4\"\n",
    "TARGET_VIDEO_PATH = \"sample_images_output/sample_video_annotated.mp4\"\n",
    "IMG_SIZE = int(VARIANTS[\"large\"][\"resolution\"])\n",
    "\n",
    "\n",
    "mask_annotator = sv.MaskAnnotator(\n",
    "    opacity=0.4,\n",
    ")\n",
    "box_annotator = sv.BoxAnnotator(\n",
    "    thickness=2,\n",
    ")\n",
    "label_annotator = sv.LabelAnnotator(\n",
    "    # text_scale=0.4,\n",
    "    # text_padding=8,\n",
    "    # text_thickness=0,\n",
    "    smart_position=True,  # <– try to avoid overlapping labels\n",
    ")\n",
    "\n",
    "\n",
    "def resize_long_side(image: Image.Image, max_size: int = 560) -> Image.Image:\n",
    "    w, h = image.size\n",
    "    if max(w, h) <= max_size:\n",
    "        return image\n",
    "    scale = max_size / float(max(w, h))\n",
    "    new_w, new_h = int(w * scale), int(h * scale)\n",
    "    return image.resize((new_w, new_h), Image.Resampling.LANCZOS)\n",
    "\n",
    "\n",
    "def callback(frame: np.ndarray, index: int) -> np.ndarray:\n",
    "    if index % 50 == 0:\n",
    "        print(f\"[cb] frame {index}, shape={frame.shape}\")\n",
    "\n",
    "    h_orig, w_orig = frame.shape[:2]\n",
    "\n",
    "    # BGR -> RGB PIL\n",
    "    rgb = frame[:, :, ::-1]\n",
    "    pil_frame = Image.fromarray(rgb)\n",
    "\n",
    "    # Resize for inference\n",
    "    send_frame = resize_long_side(pil_frame, IMG_SIZE)\n",
    "    w_inf, h_inf = send_frame.size\n",
    "\n",
    "    # Scale from inference coords -> original coords\n",
    "    scale_x = w_orig / w_inf\n",
    "    scale_y = h_orig / h_inf\n",
    "\n",
    "    # JPEG buffer\n",
    "    buf = BytesIO()\n",
    "    send_frame.save(buf, format=\"JPEG\", quality=90)\n",
    "\n",
    "    payload = {\n",
    "        \"image\": base64.b64encode(buf.getvalue()).decode(\"utf-8\"),\n",
    "        \"confidence\": 0.25,\n",
    "    }\n",
    "\n",
    "    response = rt.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        ContentType=\"application/json\",\n",
    "        Body=json.dumps(payload),\n",
    "    )\n",
    "    result = json.loads(response[\"Body\"].read().decode())\n",
    "\n",
    "    detections_raw = result[\"detections\"]\n",
    "\n",
    "    if not detections_raw:\n",
    "        return frame\n",
    "\n",
    "    # RF‑DETR: box = [x1, y1, x2, y2] in resized space\n",
    "    boxes_orig = []\n",
    "    for d in detections_raw:\n",
    "        x1, y1, x2, y2 = d[\"box\"]\n",
    "        boxes_orig.append(\n",
    "            [\n",
    "                x1 * scale_x,\n",
    "                y1 * scale_y,\n",
    "                x2 * scale_x,\n",
    "                y2 * scale_y,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    detections = sv.Detections(\n",
    "        xyxy=np.array(boxes_orig),\n",
    "        class_id=np.array([d[\"class_id\"] for d in detections_raw]),\n",
    "        confidence=np.array([d[\"confidence\"] for d in detections_raw]),\n",
    "    )\n",
    "\n",
    "    labels = [f\"{d['label']} {d['confidence'] * 100:.1f}%\" for d in detections_raw]\n",
    "\n",
    "    annotated = box_annotator.annotate(\n",
    "        scene=frame.copy(),\n",
    "        detections=detections,\n",
    "    )\n",
    "    annotated = label_annotator.annotate(\n",
    "        scene=annotated,\n",
    "        detections=detections,\n",
    "        labels=labels,\n",
    "    )\n",
    "\n",
    "    return annotated\n",
    "\n",
    "\n",
    "print(\"Starting video processing...\")\n",
    "sv.process_video(\n",
    "    source_path=SOURCE_VIDEO_PATH,\n",
    "    target_path=TARGET_VIDEO_PATH,\n",
    "    callback=callback,\n",
    "    show_progress=True,  # supported in recent versions [web:22][web:26]\n",
    ")\n",
    "print(\"Processing complete.\")\n",
    "print(\"Final file size (bytes):\", os.path.getsize(TARGET_VIDEO_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093b63f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
