{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa402be7",
   "metadata": {},
   "source": [
    "# Deploying RF-DETR Model to SageMaker Using PyTorch\n",
    "\n",
    "**Designed for use in Amazon SageMaker Studio Environment**\n",
    "\n",
    "\n",
    "This notebook demonstrates end-to-end deployment of the RF-DETR object detection model to Amazon SageMaker using PyTorch. The workflow includes using the pre-trained rf-detr-large checkpoint, creating custom inference code, packaging the model artifact, and deploying to a real-time SageMaker endpoint with GPU acceleration.\n",
    "\n",
    "This Notebook has been successfully tested in Amazon SageMaker Studio.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2aea411",
   "metadata": {},
   "source": [
    "## Load Required Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efd60605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Use this version of pip to avoid compatibility issues\n",
    "%pip install pip==24.0 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "323833bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements-deploy.txt -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac0a638",
   "metadata": {},
   "source": [
    "## Initialize SageMaker Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9b1af58-315f-4b73-b8b6-e4d5b185899b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "RoleArn: arn:aws:iam::676164205626:role/service-role/AmazonSageMaker-ExecutionRole-20221107T192794\n",
      "Region: us-east-1\n",
      "Default S3 Bucket: sagemaker-us-east-1-676164205626\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import os\n",
    "\n",
    "session = sagemaker.Session()\n",
    "bucket = session.default_bucket()\n",
    "region = session.boto_region_name\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except Exception:\n",
    "    role = os.environ.get(\"SAGEMAKER_ROLE\")\n",
    "    if role is None:\n",
    "        raise RuntimeError(\n",
    "            \"SageMaker role not found. Set SAGEMAKER_ROLE env var when running locally.\"\n",
    "        )\n",
    "\n",
    "print(f\"RoleArn: {role}\")\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"Default S3 Bucket: {bucket}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9975c33e",
   "metadata": {},
   "source": [
    "## Compress and Copy Model Model Artifacts to Amazon S3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82dced4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "VARIANTS = {\n",
    "    \"nano\": {\"file\": \"rf-detr-nano.pth\", \"type\": \"rfdetr-nano\", \"resolution\": \"384\"},\n",
    "    \"small\": {\"file\": \"rf-detr-small.pth\", \"type\": \"rfdetr-small\", \"resolution\": \"512\"},\n",
    "    \"medium\": {\n",
    "        \"file\": \"rf-detr-medium.pth\",\n",
    "        \"type\": \"rfdetr-medium\",\n",
    "        \"resolution\": \"576\",\n",
    "    },\n",
    "    \"base\": {\"file\": \"rf-detr-base.pth\", \"type\": \"rfdetr-base\", \"resolution\": \"560\"},\n",
    "    \"large\": {\"file\": \"rf-detr-large.pth\", \"type\": \"rfdetr-large\", \"resolution\": \"560\"},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86de04bf-7ec0-4b5d-9c1e-81b9b539e245",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from rfdetr import RFDETRLarge\n",
    "\n",
    "# Download the model 1x\n",
    "model = RFDETRLarge()\n",
    "print(model.model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fae9d58-e08a-4931-9228-50f72b7247bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Programmatically create a model artifact (tar.gz) containing the chosen weights and upload to S3\n",
    "import tarfile\n",
    "import os\n",
    "\n",
    "# Choose the weight file you want to package (one of the downloaded files)\n",
    "model_name = VARIANTS[\"large\"][\"file\"]\n",
    "weights_path = os.path.join(os.getcwd(), model_name)\n",
    "if not os.path.exists(weights_path):\n",
    "    raise FileNotFoundError(f\"Weights not found: {weights_path}\")\n",
    "\n",
    "artifact_path = \"model.tar.gz\"\n",
    "with tarfile.open(artifact_path, \"w:gz\") as tar:\n",
    "    tar.add(weights_path, arcname=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd6fdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Upload to SageMaker session default S3 bucket\n",
    "model_s3_path = session.upload_data(\n",
    "    path=artifact_path, bucket=bucket, key_prefix=\"pytorch_models_rf-detr\"\n",
    ")\n",
    "print(f\"Uploaded model artifact to: {model_s3_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c47ab7",
   "metadata": {},
   "source": [
    "## Create the SageMaker Real-Time Endpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c38cdc-f22e-497d-8f12-c2929680de6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from datetime import datetime, timezone\n",
    "import sagemaker\n",
    "\n",
    "\n",
    "# Reuse session from upload cell or create new one\n",
    "if \"session\" not in globals():\n",
    "    session = sagemaker.Session()\n",
    "model_destination = f\"s3://{bucket}/pytorch_models_rf-detr/model.tar.gz\"\n",
    "\n",
    "# Prefer the artifact uploaded earlier by the notebook (model_s3_path),\n",
    "# Otherwise, fall back to the constructed model_destination\n",
    "model_data = globals().get(\"model_s3_path\", model_destination)\n",
    "\n",
    "print(f\"Using model_data: {model_data}\")\n",
    "\n",
    "image_uri = f\"763104351884.dkr.ecr.{region}.amazonaws.com/pytorch-inference:2.6.0-gpu-py312-cu124-ubuntu22.04-sagemaker-v1.56\"\n",
    "\n",
    "pytorch_model = PyTorchModel(\n",
    "    model_data=model_data,\n",
    "    role=role,\n",
    "    image_uri=image_uri,\n",
    "    entry_point=\"inference.py\",\n",
    "    source_dir=\"code\",\n",
    "    env={\n",
    "        \"RFDETR_VARIANT\": \"large\",\n",
    "        \"RFDETR_CONF\": \"0.25\",\n",
    "    },\n",
    ")\n",
    "\n",
    "instance_type = \"ml.g4dn.xlarge\"\n",
    "\n",
    "endpoint_name = (\n",
    "    \"rfdetr-\"\n",
    "    + VARIANTS[\"large\"][\"type\"]\n",
    "    + \"-pytorch-\"\n",
    "    + str(datetime.now(timezone.utc).strftime(\"%Y-%m-%d-%H-%M-%S-%f\"))\n",
    ")\n",
    "\n",
    "print(f\"Deploying to endpoint: {endpoint_name}\")\n",
    "\n",
    "# Should take about 7-8 minutes to deploy\n",
    "predictor = pytorch_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    endpoint_name=endpoint_name,\n",
    "    deserializer=JSONDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223014f4-31ce-4006-8bc0-36b7fbe4e213",
   "metadata": {},
   "source": [
    "## Real-time Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c3cf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You SageMaker real-time endpoint name\n",
    "ENDPOINT_NAME = \"rfdetr-large-pytorch-2025-12-29-21-19-19-286016\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5826bc",
   "metadata": {},
   "source": [
    "### Simple Object Detection\n",
    "\n",
    "Perform real-time inference on the sample image directory and display the results with bounding box visualization. We are not passing any additional optional parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19da95f7-92ad-47ca-b9ba-f7a56880833f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sagemaker.serializers import IdentitySerializer\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from io import BytesIO\n",
    "import json\n",
    "import random\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Use the predictor from the deploy cell above (already has correct endpoint and session)\n",
    "# If reconnecting to an existing endpoint, uncomment and set endpoint_name:\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "predictor = Predictor(\n",
    "    endpoint_name=ENDPOINT_NAME,\n",
    "    sagemaker_session=session,\n",
    "    deserializer=JSONDeserializer(),\n",
    ")\n",
    "\n",
    "predictor.serializer = IdentitySerializer(content_type=\"image/jpeg\")\n",
    "\n",
    "base_dir = \"sample_images\"\n",
    "out_dir = \"sample_images_output\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "image_paths = sorted(\n",
    "    glob.glob(os.path.join(base_dir, \"*.jpg\"))\n",
    "    + glob.glob(os.path.join(base_dir, \"*.jpeg\"))\n",
    "    + glob.glob(os.path.join(base_dir, \"*.png\"))\n",
    ")\n",
    "\n",
    "if not image_paths:\n",
    "    raise FileNotFoundError(f\"No images found in {base_dir}\")\n",
    "\n",
    "print(f\"Found {len(image_paths)} images in {base_dir}\")\n",
    "\n",
    "font = ImageFont.load_default(size=24)\n",
    "IMG_SIZE = int(VARIANTS[\"large\"][\"resolution\"])\n",
    "\n",
    "\n",
    "def resize_long_side(image: Image.Image, max_size: int = 640) -> Image.Image:\n",
    "    w, h = image.size\n",
    "    long_side = max(w, h)\n",
    "    if long_side <= max_size:\n",
    "        return image  # no upscaling\n",
    "    scale = max_size / float(long_side)\n",
    "    new_w, new_h = int(w * scale), int(h * scale)\n",
    "    return image.resize((new_w, new_h), Image.Resampling.LANCZOS)\n",
    "\n",
    "\n",
    "for image_path in image_paths:\n",
    "    try:\n",
    "        orig_image = Image.open(image_path).convert(\"RGB\")\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping unreadable image: {image_path} - {e}\")\n",
    "        continue\n",
    "\n",
    "    # Downscale client-side: long side = 560, keep aspect ratio\n",
    "    send_image = resize_long_side(orig_image, IMG_SIZE)\n",
    "\n",
    "    buffer = BytesIO()\n",
    "    send_image.save(buffer, format=\"JPEG\", quality=90)\n",
    "    payload = buffer.getvalue()\n",
    "\n",
    "    result = predictor.predict(payload)\n",
    "    # print(json.dumps(result, indent=2))\n",
    "\n",
    "    # ASSUMPTION: boxes are in the coordinates of send_image\n",
    "    draw = ImageDraw.Draw(orig_image)\n",
    "\n",
    "    send_w, send_h = send_image.size\n",
    "    orig_w, orig_h = orig_image.size\n",
    "    x_ratio = orig_w / send_w\n",
    "    y_ratio = orig_h / send_h\n",
    "\n",
    "    for det in result.get(\"detections\", []):\n",
    "        x1, y1, x2, y2 = det[\"box\"]\n",
    "        conf = det[\"confidence\"]\n",
    "        label = det[\"label\"]\n",
    "\n",
    "        # Scale from send_image coords back to original\n",
    "        x1, x2 = int(x_ratio * x1), int(x_ratio * x2)\n",
    "        y1, y2 = int(y_ratio * y1), int(y_ratio * y2)\n",
    "\n",
    "        # color ranges: 10, 255 full, 10-100 dark, 150-225 light\n",
    "        color = (\n",
    "            random.randint(10, 100),\n",
    "            random.randint(10, 100),\n",
    "            random.randint(10, 100),\n",
    "        )\n",
    "\n",
    "        draw.rectangle([(x1, y1), (x2, y2)], outline=color, width=3)\n",
    "\n",
    "        text = f\"{label} ({int(conf * 100)}%)\"\n",
    "        draw.text((x1, max(0, y1 - 30)), text, fill=color, font=font)\n",
    "\n",
    "    base_name = os.path.basename(image_path)\n",
    "    name, ext = os.path.splitext(base_name)\n",
    "    out_path = os.path.join(out_dir, f\"{name}_detected{ext}\")\n",
    "    orig_image.save(out_path, quality=95)\n",
    "    print(f\"Saved: {out_path}\")\n",
    "\n",
    "print(f\"Done. {len(image_paths)} images processed; results in: {out_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38iauwr549",
   "metadata": {},
   "source": [
    "### Real-time Inference with JSON Format and Dynamic Confidence Threshold\n",
    "\n",
    "The inference endpoint supports a JSON request format that allows you to dynamically specify the confidence threshold per request. This is useful for:\n",
    "\n",
    "- Testing different confidence thresholds without redeploying\n",
    "- Adjusting sensitivity based on use case\n",
    "- A/B testing different threshold values\n",
    "\n",
    "**JSON Request Format:**\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"image\": \"base64_encoded_image_data\",\n",
    "  \"confidence\": 0.35 // Optional: defaults to 0.25 if not provided\n",
    "}\n",
    "```\n",
    "\n",
    "The example below demonstrates how to use the JSON format with different confidence thresholds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834ntzewrm",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import json\n",
    "import base64\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Connect to the existing endpoint\n",
    "predictor = Predictor(\n",
    "    endpoint_name=ENDPOINT_NAME,\n",
    "    sagemaker_session=session,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer(),\n",
    ")\n",
    "\n",
    "# Load a sample image\n",
    "base_dir = \"sample_images\"\n",
    "image_paths = glob.glob(os.path.join(base_dir, \"*.jpg\"))[:1]  # Test with first image\n",
    "IMG_SIZE = int(VARIANTS[\"large\"][\"resolution\"])\n",
    "\n",
    "if not image_paths:\n",
    "    raise FileNotFoundError(f\"No images found in {base_dir}\")\n",
    "\n",
    "image_path = image_paths[0]\n",
    "print(f\"Testing with image: {image_path}\")\n",
    "\n",
    "# Load and prepare image\n",
    "orig_image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "\n",
    "# Downscale client-side: long side = 560, keep aspect ratio\n",
    "def resize_long_side(image: Image.Image, max_size: int = 560) -> Image.Image:\n",
    "    w, h = image.size\n",
    "    long_side = max(w, h)\n",
    "    if long_side <= max_size:\n",
    "        return image\n",
    "    scale = max_size / float(long_side)\n",
    "    new_w, new_h = int(w * scale), int(h * scale)\n",
    "    return image.resize((new_w, new_h), Image.Resampling.LANCZOS)\n",
    "\n",
    "\n",
    "send_image = resize_long_side(orig_image, IMG_SIZE)\n",
    "\n",
    "buffer = BytesIO()\n",
    "send_image.save(buffer, format=\"JPEG\", quality=90)\n",
    "image_bytes = buffer.getvalue()\n",
    "\n",
    "# Test with different confidence thresholds\n",
    "confidence_thresholds = [0.25, 0.5, 0.90, 0.94]\n",
    "\n",
    "print(f\"\\nTesting with different confidence thresholds:\\n\")\n",
    "\n",
    "for confidence in confidence_thresholds:\n",
    "    # Create JSON payload with base64-encoded image and confidence\n",
    "    payload = {\n",
    "        \"image\": base64.b64encode(image_bytes).decode(\"utf-8\"),\n",
    "        \"confidence\": confidence,\n",
    "    }\n",
    "\n",
    "    # Send request\n",
    "    result = predictor.predict(payload)\n",
    "\n",
    "    detection_count = result.get(\"metadata\", {}).get(\"count\", 0)\n",
    "    inference_time = result.get(\"metadata\", {}).get(\"inference_time_ms\", \"N/A\")\n",
    "\n",
    "    print(\n",
    "        f\"Confidence: {confidence:.2f} | Detections: {detection_count:2d} | Inference time: {inference_time} ms\"\n",
    "    )\n",
    "\n",
    "    # Print first detection for reference\n",
    "    if result.get(\"detections\"):\n",
    "        first_det = result[\"detections\"][0]\n",
    "        print(\n",
    "            f\"  → First detection: {first_det['label']} ({first_det['confidence']:.3f})\"\n",
    "        )\n",
    "\n",
    "print(\n",
    "    f\"\\nNote: Higher confidence thresholds typically result in fewer but more confident detections.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hlrkrm7b7u8",
   "metadata": {},
   "source": [
    "### Advanced Filtering: Classes, Max Detections, and Minimum Box Area\n",
    "\n",
    "In addition to dynamic confidence thresholds, the endpoint supports powerful post-processing filters:\n",
    "\n",
    "1. **`classes`**: Filter to specific object classes (e.g., only \"person\", \"car\"). Set to `null` to return all classes.\n",
    "2. **`max_detections`**: Limit the number of detections returned (returns top N by confidence)\n",
    "3. **`min_box_area`**: Filter out small detections below a minimum bounding box area (in pixels²)\n",
    "\n",
    "These filters are applied after inference, so they don't affect inference speed but can significantly reduce response payload size.\n",
    "\n",
    "**Example JSON Request:**\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"image\": \"base64_encoded_image\",\n",
    "  \"confidence\": 0.3,\n",
    "  \"classes\": [\"person\", \"car\"], // Only return people and cars (or null for all)\n",
    "  \"max_detections\": 10, // Return top 10 detections only\n",
    "  \"min_box_area\": 1000 // Filter out boxes smaller than 1000 px²\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vaho9chqgc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import json\n",
    "import base64\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Connect to the existing endpoint\n",
    "predictor = Predictor(\n",
    "    endpoint_name=ENDPOINT_NAME,\n",
    "    sagemaker_session=session,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer(),\n",
    ")\n",
    "\n",
    "# Load a sample image\n",
    "base_dir = \"sample_images\"\n",
    "image_paths = glob.glob(os.path.join(base_dir, \"*.jpg\"))[:1]\n",
    "IMG_SIZE = int(VARIANTS[\"large\"][\"resolution\"])\n",
    "\n",
    "if not image_paths:\n",
    "    raise FileNotFoundError(f\"No images found in {base_dir}\")\n",
    "\n",
    "image_path = image_paths[0]\n",
    "print(f\"Testing with image: {image_path}\\n\")\n",
    "\n",
    "# Load and prepare image\n",
    "orig_image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "\n",
    "def resize_long_side(image: Image.Image, max_size: int = 560) -> Image.Image:\n",
    "    w, h = image.size\n",
    "    long_side = max(w, h)\n",
    "    if long_side <= max_size:\n",
    "        return image\n",
    "    scale = max_size / float(long_side)\n",
    "    new_w, new_h = int(w * scale), int(h * scale)\n",
    "    return image.resize((new_w, new_h), Image.Resampling.LANCZOS)\n",
    "\n",
    "\n",
    "send_image = resize_long_side(orig_image, IMG_SIZE)\n",
    "\n",
    "buffer = BytesIO()\n",
    "send_image.save(buffer, format=\"JPEG\", quality=90)\n",
    "image_bytes = buffer.getvalue()\n",
    "image_b64 = base64.b64encode(image_bytes).decode(\"utf-8\")\n",
    "\n",
    "# Example 1: Return all classes (no class filtering)\n",
    "print(\"=\" * 80)\n",
    "print(\"Example 1: Return all classes (classes=null)\")\n",
    "print(\"=\" * 80)\n",
    "payload1 = {\n",
    "    \"image\": image_b64,\n",
    "    \"confidence\": 0.25,\n",
    "    \"classes\": None,  # Explicitly request all classes\n",
    "}\n",
    "result1 = predictor.predict(payload1)\n",
    "print(f\"Detections: {result1['metadata']['count']}\")\n",
    "print(f\"Classes found: {list(set([d['label'] for d in result1['detections']]))}\")\n",
    "print()\n",
    "\n",
    "# Example 2: Filter to specific classes only\n",
    "print(\"=\" * 80)\n",
    "print(\"Example 2: Filter to specific classes (scissors, bottle, knife)\")\n",
    "print(\"=\" * 80)\n",
    "payload2 = {\n",
    "    \"image\": image_b64,\n",
    "    \"confidence\": 0.25,\n",
    "    \"classes\": [\"scissors\", \"bottle\", \"knife\"],\n",
    "}\n",
    "result2 = predictor.predict(payload2)\n",
    "print(f\"Detections: {result2['metadata']['count']}\")\n",
    "print(f\"Classes found: {list(set([d['label'] for d in result1['detections']]))}\")\n",
    "if result2[\"metadata\"].get(\"applied_filters\"):\n",
    "    print(f\"Applied filters: {result2['metadata']['applied_filters']}\")\n",
    "print()\n",
    "\n",
    "# Example 3: Limit to top 5 detections\n",
    "print(\"=\" * 80)\n",
    "print(\"Example 3: Limit to top 5 detections by confidence\")\n",
    "print(\"=\" * 80)\n",
    "payload3 = {\n",
    "    \"image\": image_b64,\n",
    "    \"confidence\": 0.2,  # Lower confidence to get more candidates\n",
    "    \"max_detections\": 5,\n",
    "}\n",
    "result3 = predictor.predict(payload3)\n",
    "print(f\"Detections returned: {result3['metadata']['count']}\")\n",
    "if result3[\"metadata\"].get(\"applied_filters\"):\n",
    "    print(f\"Applied filters: {result3['metadata']['applied_filters']}\")\n",
    "if result3[\"detections\"]:\n",
    "    print(\n",
    "        f\"Top detection: {result3['detections'][0]['label']} ({result3['detections'][0]['confidence']:.3f})\"\n",
    "    )\n",
    "print()\n",
    "\n",
    "# Example 4: Filter by minimum box area\n",
    "print(\"=\" * 80)\n",
    "print(\"Example 4: Filter out small detections (min_box_area=8000 px²)\")\n",
    "print(\"=\" * 80)\n",
    "payload4 = {\"image\": image_b64, \"confidence\": 0.25, \"min_box_area\": 8000}\n",
    "result4 = predictor.predict(payload4)\n",
    "print(f\"Detections (large objects only): {result4['metadata']['count']}\")\n",
    "if result4[\"detections\"]:\n",
    "    for det in result4[\"detections\"]:\n",
    "        print(\n",
    "            f\"  - {det['label']}: {det['area']:.1f} px² (conf: {det['confidence']:.3f})\"\n",
    "        )\n",
    "if result4[\"metadata\"].get(\"applied_filters\"):\n",
    "    print(f\"Applied filters: {result4['metadata']['applied_filters']}\")\n",
    "print()\n",
    "\n",
    "# Example 5: Combine all filters\n",
    "print(\"=\" * 80)\n",
    "print(\"Example 5: Combine all filters\")\n",
    "print(\"=\" * 80)\n",
    "payload5 = {\n",
    "    \"image\": image_b64,\n",
    "    \"confidence\": 0.2,\n",
    "    \"classes\": [\"scissors\", \"bottle\", \"knife\"],\n",
    "    \"max_detections\": 3,\n",
    "    \"min_box_area\": 1000,\n",
    "}\n",
    "result5 = predictor.predict(payload5)\n",
    "print(f\"Final detections: {result5['metadata']['count']}\")\n",
    "print(f\"Applied filters: {result5['metadata'].get('applied_filters', {})}\")\n",
    "for det in result5[\"detections\"]:\n",
    "    print(\n",
    "        f\"  - {det['label']}: area={det['area']:.1f}px², conf={det['confidence']:.3f}\"\n",
    "    )\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Summary: Filters allow you to customize responses without redeploying!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6921b8-ceda-461e-85c2-ea6eb01b7fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import base64\n",
    "import supervision as sv\n",
    "import numpy as np\n",
    "\n",
    "# Load and prepare image\n",
    "orig_image = Image.open(\"objects_small.jpg\").convert(\"RGB\")\n",
    "\n",
    "IMG_SIZE = int(VARIANTS[\"large\"][\"resolution\"])\n",
    "\n",
    "\n",
    "def resize_long_side(image: Image.Image, max_size: int = 560) -> Image.Image:\n",
    "    w, h = image.size\n",
    "    long_side = max(w, h)\n",
    "    if long_side <= max_size:\n",
    "        return image\n",
    "    scale = max_size / float(long_side)\n",
    "    new_w, new_h = int(w * scale), int(h * scale)\n",
    "    return image.resize((new_w, new_h), Image.Resampling.LANCZOS)\n",
    "\n",
    "\n",
    "send_frame = resize_long_side(orig_image, IMG_SIZE)\n",
    "buffer = BytesIO()\n",
    "send_frame.save(buffer, format=\"JPEG\", quality=90)\n",
    "\n",
    "# Calculate scaling factors for box coordinates\n",
    "scale_x = orig_image.size[0] / send_frame.size[0]\n",
    "scale_y = orig_image.size[1] / send_frame.size[1]\n",
    "\n",
    "# Prepare and send request\n",
    "payload = {\n",
    "    \"image\": base64.b64encode(buffer.getvalue()).decode(\"utf-8\"),\n",
    "    \"confidence\": 0.5,\n",
    "}\n",
    "\n",
    "predictor = Predictor(\n",
    "    endpoint_name=ENDPOINT_NAME,\n",
    "    sagemaker_session=session,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer(),\n",
    ")\n",
    "\n",
    "result = predictor.predict(payload)\n",
    "print(f\"Detections: {result['metadata']['count']}\")\n",
    "\n",
    "# Rescale boxes to original image coordinates\n",
    "boxes_rescaled = [\n",
    "    [x1 * scale_x, y1 * scale_y, x2 * scale_x, y2 * scale_y]\n",
    "    for x1, y1, x2, y2 in [d[\"box\"] for d in result[\"detections\"]]\n",
    "]\n",
    "\n",
    "# Create detections and labels\n",
    "detections = sv.Detections(\n",
    "    xyxy=np.array(boxes_rescaled),\n",
    "    class_id=np.array([d[\"class_id\"] for d in result[\"detections\"]]),\n",
    "    confidence=np.array([d[\"confidence\"] for d in result[\"detections\"]]),\n",
    ")\n",
    "\n",
    "labels = [f\"{d['label']} {d['confidence'] * 100:.1f}%\" for d in result[\"detections\"]]\n",
    "\n",
    "# Annotate image\n",
    "annotated = sv.BoxAnnotator().annotate(\n",
    "    sv.LabelAnnotator(smart_position=True).annotate(\n",
    "        orig_image, detections=detections, labels=labels\n",
    "    ),\n",
    "    detections=detections,\n",
    ")\n",
    "\n",
    "# Save result\n",
    "annotated.save(\"annotated_output.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7969ca",
   "metadata": {},
   "source": [
    "## Video Object Detection with Amazon SageMaker\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4821036",
   "metadata": {},
   "source": [
    "### Simple Video Object Detections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb2c1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import supervision as sv\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "SOURCE_VIDEO_PATH = \"sample_video.mp4\"\n",
    "TARGET_VIDEO_PATH = \"sample_video_annotated.mp4\"\n",
    "IMG_SIZE = int(VARIANTS[\"large\"][\"resolution\"])\n",
    "\n",
    "# Single, reused SageMaker predictor\n",
    "predictor = Predictor(\n",
    "    endpoint_name=ENDPOINT_NAME,\n",
    "    sagemaker_session=session,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer(),\n",
    ")\n",
    "\n",
    "mask_annotator = sv.MaskAnnotator(\n",
    "    opacity=0.4,\n",
    ")\n",
    "box_annotator = sv.BoxAnnotator(\n",
    "    thickness=2,\n",
    ")\n",
    "label_annotator = sv.LabelAnnotator(\n",
    "    # text_scale=0.4,\n",
    "    # text_padding=8,\n",
    "    # text_thickness=0,\n",
    "    smart_position=True,  # <– try to avoid overlapping labels\n",
    ")\n",
    "\n",
    "\n",
    "def resize_long_side(image: Image.Image, max_size: int = 560) -> Image.Image:\n",
    "    w, h = image.size\n",
    "    if max(w, h) <= max_size:\n",
    "        return image\n",
    "    scale = max_size / float(max(w, h))\n",
    "    new_w, new_h = int(w * scale), int(h * scale)\n",
    "    return image.resize((new_w, new_h), Image.Resampling.LANCZOS)\n",
    "\n",
    "\n",
    "def callback(frame: np.ndarray, index: int) -> np.ndarray:\n",
    "    if index % 50 == 0:\n",
    "        print(f\"[cb] frame {index}, shape={frame.shape}\")\n",
    "\n",
    "    h_orig, w_orig = frame.shape[:2]\n",
    "\n",
    "    # BGR -> RGB PIL\n",
    "    rgb = frame[:, :, ::-1]\n",
    "    pil_frame = Image.fromarray(rgb)\n",
    "\n",
    "    # Resize for inference\n",
    "    send_frame = resize_long_side(pil_frame, IMG_SIZE)\n",
    "    w_inf, h_inf = send_frame.size\n",
    "\n",
    "    # Scale from inference coords -> original coords\n",
    "    scale_x = w_orig / w_inf\n",
    "    scale_y = h_orig / h_inf\n",
    "\n",
    "    # JPEG buffer\n",
    "    buf = BytesIO()\n",
    "    send_frame.save(buf, format=\"JPEG\", quality=90)\n",
    "\n",
    "    payload = {\n",
    "        \"image\": base64.b64encode(buf.getvalue()).decode(\"utf-8\"),\n",
    "        \"confidence\": 0.25,\n",
    "    }\n",
    "\n",
    "    result = predictor.predict(payload)\n",
    "    detections_raw = result[\"detections\"]\n",
    "\n",
    "    if not detections_raw:\n",
    "        return frame\n",
    "\n",
    "    # RF‑DETR: box = [x1, y1, x2, y2] in resized space\n",
    "    boxes_orig = []\n",
    "    for d in detections_raw:\n",
    "        x1, y1, x2, y2 = d[\"box\"]\n",
    "        boxes_orig.append(\n",
    "            [\n",
    "                x1 * scale_x,\n",
    "                y1 * scale_y,\n",
    "                x2 * scale_x,\n",
    "                y2 * scale_y,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    detections = sv.Detections(\n",
    "        xyxy=np.array(boxes_orig),\n",
    "        class_id=np.array([d[\"class_id\"] for d in detections_raw]),\n",
    "        confidence=np.array([d[\"confidence\"] for d in detections_raw]),\n",
    "    )\n",
    "\n",
    "    labels = [f\"{d['label']} {d['confidence'] * 100:.1f}%\" for d in detections_raw]\n",
    "\n",
    "    annotated = box_annotator.annotate(\n",
    "        scene=frame.copy(),\n",
    "        detections=detections,\n",
    "    )\n",
    "    annotated = label_annotator.annotate(\n",
    "        scene=annotated,\n",
    "        detections=detections,\n",
    "        labels=labels,\n",
    "    )\n",
    "\n",
    "    return annotated\n",
    "\n",
    "\n",
    "print(\"Starting video processing...\")\n",
    "sv.process_video(\n",
    "    source_path=SOURCE_VIDEO_PATH,\n",
    "    target_path=TARGET_VIDEO_PATH,\n",
    "    callback=callback,\n",
    "    show_progress=True,  # supported in recent versions [web:22][web:26]\n",
    ")\n",
    "print(\"Processing complete.\")\n",
    "print(\"Final file size (bytes):\", os.path.getsize(TARGET_VIDEO_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45252c9d",
   "metadata": {},
   "source": [
    "### Video Object Detection with Counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0d56e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import supervision as sv\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "SOURCE_VIDEO_PATH = \"sample_video.mp4\"\n",
    "TARGET_VIDEO_PATH = \"sample_video_annotated.mp4\"\n",
    "IMG_SIZE = int(VARIANTS[\"large\"][\"resolution\"])\n",
    "\n",
    "# Single, reused SageMaker predictor\n",
    "predictor = Predictor(\n",
    "    endpoint_name=ENDPOINT_NAME,\n",
    "    sagemaker_session=session,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer(),\n",
    ")\n",
    "\n",
    "mask_annotator = sv.MaskAnnotator(\n",
    "    opacity=0.4,\n",
    ")\n",
    "box_annotator = sv.BoxAnnotator(\n",
    "    thickness=2,\n",
    ")\n",
    "label_annotator = sv.LabelAnnotator(\n",
    "    # text_scale=0.4,\n",
    "    # text_padding=8,\n",
    "    # text_thickness=0,\n",
    "    smart_position=True,  # <– try to avoid overlapping labels\n",
    ")\n",
    "\n",
    "\n",
    "def resize_long_side(image: Image.Image, max_size: int = 560) -> Image.Image:\n",
    "    w, h = image.size\n",
    "    if max(w, h) <= max_size:\n",
    "        return image\n",
    "    scale = max_size / float(max(w, h))\n",
    "    new_w, new_h = int(w * scale), int(h * scale)\n",
    "    return image.resize((new_w, new_h), Image.Resampling.LANCZOS)\n",
    "\n",
    "\n",
    "def callback(frame: np.ndarray, index: int) -> np.ndarray:\n",
    "    if index % 50 == 0:\n",
    "        print(f\"[cb] frame {index}, shape={frame.shape}\")\n",
    "\n",
    "    h_orig, w_orig = frame.shape[:2]\n",
    "\n",
    "    # BGR -> RGB PIL\n",
    "    rgb = frame[:, :, ::-1]\n",
    "    pil_frame = Image.fromarray(rgb)\n",
    "\n",
    "    # Resize for inference\n",
    "    send_frame = resize_long_side(pil_frame, IMG_SIZE)\n",
    "    w_inf, h_inf = send_frame.size\n",
    "\n",
    "    # Scale from inference coords -> original coords\n",
    "    scale_x = w_orig / w_inf\n",
    "    scale_y = h_orig / h_inf\n",
    "\n",
    "    # JPEG buffer\n",
    "    buf = BytesIO()\n",
    "    send_frame.save(buf, format=\"JPEG\", quality=90)\n",
    "\n",
    "    payload = {\n",
    "        \"image\": base64.b64encode(buf.getvalue()).decode(\"utf-8\"),\n",
    "        \"confidence\": 0.5,\n",
    "        # \"classes\": [\"person\"],\n",
    "    }\n",
    "\n",
    "    result = predictor.predict(payload)\n",
    "    detections_raw = result[\"detections\"]\n",
    "\n",
    "    if not detections_raw:\n",
    "        return frame\n",
    "\n",
    "    # RF‑DETR: box = [x1, y1, x2, y2] in resized space\n",
    "    boxes_orig = []\n",
    "    for d in detections_raw:\n",
    "        x1, y1, x2, y2 = d[\"box\"]\n",
    "        boxes_orig.append(\n",
    "            [\n",
    "                x1 * scale_x,\n",
    "                y1 * scale_y,\n",
    "                x2 * scale_x,\n",
    "                y2 * scale_y,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    detections = sv.Detections(\n",
    "        xyxy=np.array(boxes_orig),\n",
    "        class_id=np.array([d[\"class_id\"] for d in detections_raw]),\n",
    "        confidence=np.array([d[\"confidence\"] for d in detections_raw]),\n",
    "    )\n",
    "\n",
    "    labels = [f\"{d['label']} {d['confidence'] * 100:.1f}%\" for d in detections_raw]\n",
    "\n",
    "    # count of each label\n",
    "    unique_labels, counts = np.unique(\n",
    "        [d[\"label\"] for d in detections_raw], return_counts=True\n",
    "    )\n",
    "    label_counts = dict(zip(unique_labels, counts))\n",
    "    # print(f\"Frame {index} label counts: {label_counts}\")\n",
    "\n",
    "    annotated = box_annotator.annotate(\n",
    "        scene=frame.copy(),\n",
    "        detections=detections,\n",
    "    )\n",
    "    annotated = label_annotator.annotate(\n",
    "        scene=annotated,\n",
    "        detections=detections,\n",
    "        labels=labels,\n",
    "    )\n",
    "\n",
    "    # Draw global count summary (cumulative)\n",
    "    y0 = 30\n",
    "    for i, (unique_labels, counts) in enumerate(label_counts.items()):\n",
    "        text = f\"{unique_labels}: {counts}\"\n",
    "        cv2.putText(\n",
    "            annotated,\n",
    "            text,\n",
    "            (20, y0 + i * 25),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.8,\n",
    "            (0, 255, 255),\n",
    "            2,\n",
    "            cv2.LINE_AA,\n",
    "        )\n",
    "\n",
    "    return annotated\n",
    "\n",
    "\n",
    "print(\"Starting video processing...\")\n",
    "sv.process_video(\n",
    "    source_path=SOURCE_VIDEO_PATH,\n",
    "    target_path=TARGET_VIDEO_PATH,\n",
    "    callback=callback,\n",
    "    show_progress=True,  # supported in recent versions [web:22][web:26]\n",
    ")\n",
    "print(\"Processing complete.\")\n",
    "print(\"Final file size (bytes):\", os.path.getsize(TARGET_VIDEO_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0b3b6e",
   "metadata": {},
   "source": [
    "### Video Object Detection with Class Replacement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06039ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import os\n",
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import supervision as sv\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "SOURCE_VIDEO_PATH = \"kling_santa_slowed_more.mp4\"\n",
    "TARGET_VIDEO_PATH = \"kling_santa_slowed_more_annotated.mp4\"\n",
    "SUMMARY_LABELS = [\"magic sleigh\", \"santa\", \"reindeer\", \"gift\", \"sack of gifts\"]\n",
    "\n",
    "# Map certain model labels to others (text shown on screen)\n",
    "LABEL_REMAP = {\n",
    "    \"boat\": \"magic sleigh\",\n",
    "    \"bench\": \"magic sleigh\",\n",
    "    \"couch\": \"magic sleigh\",\n",
    "    \"train\": \"magic sleigh\",\n",
    "    \"person\": \"santa\",\n",
    "    \"teddy bear\": \"santa\",\n",
    "    \"sheep\": \"reindeer\",\n",
    "    \"dog\": \"reindeer\",\n",
    "    \"cow\": \"reindeer\",\n",
    "    \"horse\": \"reindeer\",\n",
    "    \"suitcase\": \"gift\",\n",
    "    \"handbag\": \"sack of gifts\",\n",
    "}\n",
    "\n",
    "# Optional: remap class IDs too (depends on your model's class index mapping)\n",
    "# Example COCO-style mapping: 17=cat, 18=dog, 19=horse, 20=sheep, etc.\n",
    "CLASS_ID_REMAP = {\n",
    "    17: 20,\n",
    "    18: 20,\n",
    "    19: 20,\n",
    "    9: 7,\n",
    "    14: 7,\n",
    "    58: 7,\n",
    "    78: 1,\n",
    "}\n",
    "\n",
    "# Single, reused SageMaker predictor\n",
    "predictor = Predictor(\n",
    "    endpoint_name=ENDPOINT_NAME,\n",
    "    sagemaker_session=session,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer(),\n",
    ")\n",
    "\n",
    "mask_annotator = sv.MaskAnnotator(\n",
    "    opacity=0.4,\n",
    ")\n",
    "box_annotator = sv.BoxAnnotator(\n",
    "    thickness=2,\n",
    ")\n",
    "label_annotator = sv.LabelAnnotator(\n",
    "    # text_scale=0.4,\n",
    "    # text_padding=8,\n",
    "    # text_thickness=0,\n",
    "    smart_position=True,  # avoid overlapping labels\n",
    ")\n",
    "\n",
    "\n",
    "def resize_long_side(image: Image.Image, max_size: int = 560) -> Image.Image:\n",
    "    w, h = image.size\n",
    "    if max(w, h) <= max_size:\n",
    "        return image\n",
    "    scale = max_size / float(max(w, h))\n",
    "    new_w, new_h = int(w * scale), int(h * scale)\n",
    "    return image.resize((new_w, new_h), Image.Resampling.LANCZOS)\n",
    "\n",
    "\n",
    "def callback(frame: np.ndarray, index: int) -> np.ndarray:\n",
    "    if index % 50 == 0:\n",
    "        print(f\"[cb] frame {index}, shape={frame.shape}\")\n",
    "\n",
    "    h_orig, w_orig = frame.shape[:2]\n",
    "\n",
    "    # BGR -> RGB PIL\n",
    "    rgb = frame[:, :, ::-1]\n",
    "    pil_frame = Image.fromarray(rgb)\n",
    "\n",
    "    # Resize for inference\n",
    "    send_frame = resize_long_side(pil_frame, 560)\n",
    "    w_inf, h_inf = send_frame.size\n",
    "\n",
    "    # Scale from inference coords -> original coords\n",
    "    scale_x = w_orig / w_inf\n",
    "    scale_y = h_orig / h_inf\n",
    "\n",
    "    # JPEG buffer\n",
    "    buf = BytesIO()\n",
    "    send_frame.save(buf, format=\"JPEG\", quality=90)\n",
    "\n",
    "    payload = {\n",
    "        \"image\": base64.b64encode(buf.getvalue()).decode(\"utf-8\"),\n",
    "        \"confidence\": 0.18,\n",
    "        \"classes\": [\n",
    "            \"person\",\n",
    "            \"train\",\n",
    "            \"boat\",\n",
    "            \"sheep\",\n",
    "            \"dog\",\n",
    "            \"cow\",\n",
    "            \"horse\",\n",
    "            \"suitcase\",\n",
    "            \"handbag\",\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    result = predictor.predict(payload)\n",
    "    detections_raw = result[\"detections\"]\n",
    "\n",
    "    if not detections_raw:\n",
    "        return frame\n",
    "\n",
    "    # --- REMAP LABELS AND CLASS IDS HERE ---------------------------------\n",
    "    for d in detections_raw:\n",
    "        # remap label text\n",
    "        if d[\"label\"] in LABEL_REMAP:\n",
    "            d[\"label\"] = LABEL_REMAP[d[\"label\"]]\n",
    "\n",
    "        # remap class_id if configured\n",
    "        cid = d.get(\"class_id\")\n",
    "        if cid in CLASS_ID_REMAP:\n",
    "            d[\"class_id\"] = CLASS_ID_REMAP[cid]\n",
    "    # ---------------------------------------------------------------------\n",
    "\n",
    "    # RF‑DETR: box = [x1, y1, x2, y2] in resized space\n",
    "    boxes_orig = []\n",
    "    for d in detections_raw:\n",
    "        x1, y1, x2, y2 = d[\"box\"]\n",
    "        boxes_orig.append(\n",
    "            [\n",
    "                x1 * scale_x,\n",
    "                y1 * scale_y,\n",
    "                x2 * scale_x,\n",
    "                y2 * scale_y,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    detections = sv.Detections(\n",
    "        xyxy=np.array(boxes_orig),\n",
    "        class_id=np.array([d[\"class_id\"] for d in detections_raw]),\n",
    "        confidence=np.array([d[\"confidence\"] for d in detections_raw]),\n",
    "    )\n",
    "\n",
    "    # Labels shown above boxes, using remapped names\n",
    "    labels = [f\"{d['label']} {d['confidence'] * 100:.1f}%\" for d in detections_raw]\n",
    "\n",
    "    # Per-frame counts, using remapped names\n",
    "    unique_labels, counts = np.unique(\n",
    "        [d[\"label\"] for d in detections_raw],\n",
    "        return_counts=True,\n",
    "    )\n",
    "    label_counts = dict(zip(unique_labels, counts))\n",
    "\n",
    "    # ensure all summary labels exist, with 0 if missing\n",
    "    full_counts = {lbl: label_counts.get(lbl, 0) for lbl in SUMMARY_LABELS}\n",
    "\n",
    "    annotated = box_annotator.annotate(\n",
    "        scene=frame.copy(),\n",
    "        detections=detections,\n",
    "    )\n",
    "    annotated = label_annotator.annotate(\n",
    "        scene=annotated,\n",
    "        detections=detections,\n",
    "        labels=labels,\n",
    "    )\n",
    "\n",
    "    # Draw per-frame count summary\n",
    "    y0 = 60\n",
    "    for i, (lbl, cnt) in enumerate(full_counts.items()):\n",
    "        text = f\"{lbl}: {cnt}\"\n",
    "        cv2.putText(\n",
    "            annotated,\n",
    "            text,\n",
    "            (40, y0 + i * 50),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            1.4,\n",
    "            (57, 254, 20),\n",
    "            2,\n",
    "            cv2.LINE_AA,\n",
    "        )\n",
    "\n",
    "    return annotated\n",
    "\n",
    "\n",
    "print(\"Starting video processing...\")\n",
    "sv.process_video(\n",
    "    source_path=SOURCE_VIDEO_PATH,\n",
    "    target_path=TARGET_VIDEO_PATH,\n",
    "    callback=callback,\n",
    "    show_progress=True,\n",
    ")\n",
    "print(\"Processing complete.\")\n",
    "print(\"Final file size (bytes):\", os.path.getsize(TARGET_VIDEO_PATH))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
